name: remnawave-local

x-common: &common
  restart: unless-stopped
  networks:
    - remnawave-network

x-logging: &logging
  logging:
    driver: json-file
    options:
      max-size: 100m
      max-file: "5"

x-env: &env
  env_file:
    - .env

services:
  remnawave:
    image: remnawave/backend:2
    container_name: remnawave
    hostname: remnawave
    <<: [*common, *logging, *env]
    ports:
      - "127.0.0.1:3000:${APP_PORT:-3000}"
      - "127.0.0.1:3001:${METRICS_PORT:-3001}"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${METRICS_PORT:-3001}/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      remnawave-db:
        condition: service_healthy
      remnawave-redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
        reservations:
          memory: 256M
          cpus: "0.25"

  remnawave-db:
    image: postgres:17.7
    container_name: remnawave-db
    hostname: remnawave-db
    <<: [*common, *logging, *env]
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - TZ=UTC
    ports:
      - "127.0.0.1:6767:5432"
    volumes:
      - remnawave-db-data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 3s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
        reservations:
          memory: 256M
          cpus: "0.25"

  remnawave-redis:
    image: valkey/valkey:8.1-alpine
    container_name: remnawave-redis
    hostname: remnawave-redis
    <<: [*common, *logging]
    ports:
      - "127.0.0.1:6379:6379"
    # INFRA-04: Valkey runs with zero persistence (--save "" --appendonly no).
    # This is intentional: Valkey is used only for cache and task queues.
    # All data is ephemeral and reconstructed on restart. No backup needed.
    # Trade-off: faster writes, lower disk I/O, but data lost on container restart.
    command: >
      valkey-server
      --save ""
      --appendonly no
      --maxmemory-policy noeviction
      --loglevel warning
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 3s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.50"
        reservations:
          memory: 64M
          cpus: "0.10"

  db-backup:
    image: prodrigestivill/postgres-backup-local:17
    container_name: db-backup
    hostname: db-backup
    <<: [*common, *logging]
    environment:
      - POSTGRES_HOST=remnawave-db
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_EXTRA_OPTS=--format=custom --compress=6
      - SCHEDULE=@daily
      - BACKUP_KEEP_DAYS=7
      - HEALTHCHECK_PORT=8080
      - TZ=UTC
    volumes:
      - ./backups/postgres:/backups
    depends_on:
      remnawave-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "sh", "-c", "find /backups -name '*.sql.gz' -o -name '*.dump' -mmin -1500 | grep -q ."]
      interval: 3600s
      timeout: 10s
      retries: 1
      start_period: 86400s

  cybervpn-worker:
    build:
      context: ../services/task-worker
      dockerfile: Dockerfile
    container_name: cybervpn-worker
    hostname: cybervpn-worker
    profiles: ["worker"]
    <<: [*common, *logging]
    env_file:
      - ../services/task-worker/.env
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-local_dev_postgres}@remnawave-db:5432/${POSTGRES_DB:-postgres}
      - REDIS_URL=redis://remnawave-redis:6379/0
      - REMNAWAVE_URL=http://remnawave:3000
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - CRYPTOBOT_TOKEN=${CRYPTOBOT_TOKEN:-}
      - ADMIN_TELEGRAM_IDS=${ADMIN_TELEGRAM_IDS:-}
      - WORKER_CONCURRENCY=${WORKER_CONCURRENCY:-2}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - METRICS_PORT=${WORKER_METRICS_PORT:-9091}
      # Override SMTP servers to use Docker network hostnames
      - SMTP_SERVERS=mailpit-1:1025,mailpit-2:1025,mailpit-3:1025
    command: ["taskiq", "worker", "src.broker:broker", "--workers", "${WORKER_CONCURRENCY:-2}", "--fs-discover"]
    ports:
      - "127.0.0.1:${WORKER_METRICS_PORT:-9091}:${WORKER_METRICS_PORT:-9091}"
    depends_on:
      remnawave-db:
        condition: service_healthy
      remnawave-redis:
        condition: service_healthy
      remnawave:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
        reservations:
          memory: 128M
          cpus: "0.25"

  cybervpn-scheduler:
    build:
      context: ../services/task-worker
      dockerfile: Dockerfile
    container_name: cybervpn-scheduler
    hostname: cybervpn-scheduler
    profiles: ["worker"]
    <<: [*common, *logging]
    env_file:
      - ../services/task-worker/.env
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-local_dev_postgres}@remnawave-db:5432/${POSTGRES_DB:-postgres}
      - REDIS_URL=redis://remnawave-redis:6379/0
      - REMNAWAVE_URL=http://remnawave:3000
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    command: ["taskiq", "scheduler", "src.broker:scheduler"]
    depends_on:
      cybervpn-worker:
        condition: service_healthy
      remnawave-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    logging:
      driver: json-file
      options:
        max-size: 20m
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.50"
        reservations:
          memory: 64M
          cpus: "0.10"

  caddy:
    image: caddy:2.9
    container_name: caddy
    hostname: caddy
    profiles: ["proxy"]
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    networks:
      - remnawave-network
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile
      - caddy-ssl-data:/data
    depends_on:
      - remnawave
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  remnawave-subscription-page:
    image: remnawave/subscription-page:latest
    container_name: remnawave-subscription-page
    hostname: remnawave-subscription-page
    profiles: ["subscription"]
    restart: unless-stopped
    env_file:
      - ./subscription/.env
    ports:
      - "127.0.0.1:3010:3010"
    networks:
      - remnawave-network
    depends_on:
      - remnawave
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3010"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  cybervpn-telegram-bot:
    build:
      context: ../services/telegram-bot
      dockerfile: Dockerfile
    container_name: cybervpn-telegram-bot
    hostname: cybervpn-telegram-bot
    profiles: ["bot"]
    <<: [*common, *logging]
    env_file:
      - ../services/telegram-bot/.env
    environment:
      - BOT_MODE=${BOT_MODE:-polling}
      - BACKEND_API_URL=http://remnawave:${APP_PORT:-3000}/api
      - BACKEND_API_KEY=${BACKEND_API_KEY:-}
      - REDIS_URL=redis://remnawave-redis:6379
      - REDIS_DB=1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_JSON_FORMAT=true
      - PROMETHEUS_ENABLED=${PROMETHEUS_ENABLED:-true}
      - PROMETHEUS_PORT=9092
    ports:
      - "127.0.0.1:${BOT_METRICS_PORT:-9092}:9092"
    depends_on:
      remnawave:
        condition: service_healthy
      remnawave-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "healthcheck.py"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.50"
        reservations:
          memory: 128M
          cpus: "0.25"

  prometheus:
    image: prom/prometheus:v3.2.1
    container_name: prometheus
    profiles: ["monitoring"]
    restart: unless-stopped
    ports:
      - "127.0.0.1:9094:9090"  # Changed from 9090 to avoid conflict with task-worker metrics
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - remnawave-network

  grafana:
    image: grafana/grafana:11.5.2
    container_name: grafana
    profiles: ["monitoring"]
    restart: unless-stopped
    ports:
      - "127.0.0.1:3002:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-grafana_local_password}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/etc/grafana/dashboards
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - remnawave-network

  alertmanager:
    image: prom/alertmanager:v0.28.1
    container_name: alertmanager
    profiles: ["monitoring"]
    restart: unless-stopped
    ports:
      - "127.0.0.1:9093:9093"
    environment:
      - ALERTMANAGER_WEBHOOK_URL=${ALERTMANAGER_WEBHOOK_URL:-https://your-telegram-relay.example.com/alert}
    entrypoint: ["/docker-entrypoint.sh"]
    command:
      - "--storage.path=/alertmanager"
      - "--web.external-url=http://localhost:9093"
    volumes:
      - ./alertmanager/alertmanager.yml.template:/etc/alertmanager/alertmanager.yml.template:ro
      - ./alertmanager/docker-entrypoint.sh:/docker-entrypoint.sh:ro
      - ./alertmanager/templates:/etc/alertmanager/templates:ro
      - alertmanager_data:/alertmanager
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - remnawave-network

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.121.0
    container_name: otel-collector
    hostname: otel-collector
    profiles: ["monitoring"]
    <<: [*common, *logging]
    command: ["--config=/etc/otelcol/otel-collector-config.yml"]
    ports:
      - "127.0.0.1:4317:4317"   # OTLP gRPC
      - "127.0.0.1:4318:4318"   # OTLP HTTP
      - "127.0.0.1:8888:8888"   # Prometheus metrics
    volumes:
      - ./otel/otel-collector-config.yml:/etc/otelcol/otel-collector-config.yml:ro
    depends_on:
      tempo:
        condition: service_healthy
      prometheus:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8888/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.50"
        reservations:
          memory: 128M
          cpus: "0.10"

  tempo:
    image: grafana/tempo:2.7.2
    container_name: tempo
    hostname: tempo
    profiles: ["monitoring"]
    <<: [*common, *logging]
    command: ["-config.file=/etc/tempo/tempo-config.yml"]
    ports:
      - "127.0.0.1:3200:3200"   # Tempo HTTP API
      - "127.0.0.1:9095:9095"   # Tempo gRPC
    volumes:
      - ./tempo/tempo-config.yml:/etc/tempo/tempo-config.yml:ro
      - tempo_data:/tempo
    depends_on:
      remnawave-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
        reservations:
          memory: 256M
          cpus: "0.25"

  loki:
    image: grafana/loki:3.4.2
    container_name: loki
    hostname: loki
    profiles: ["monitoring"]
    <<: [*common, *logging]
    ports:
      - "127.0.0.1:3100:3100"
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - ./loki/rules:/loki/rules:ro
      - loki_data:/loki
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.50"
        reservations:
          memory: 128M
          cpus: "0.10"

  promtail:
    image: grafana/promtail:3.4.2
    container_name: promtail
    hostname: promtail
    profiles: ["monitoring"]
    <<: [*common, *logging]
    command: -config.file=/etc/promtail/promtail-config.yml
    volumes:
      - ./loki/promtail-config.yml:/etc/promtail/promtail-config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/log:/var/log:ro
      - promtail_positions:/tmp
    depends_on:
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.50"
        reservations:
          memory: 64M
          cpus: "0.10"

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.17.0
    container_name: postgres-exporter
    hostname: postgres-exporter
    profiles: ["monitoring"]
    <<: [*common, *logging]
    environment:
      - DATA_SOURCE_NAME=postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-local_dev_postgres}@remnawave-db:5432/${POSTGRES_DB:-postgres}?sslmode=disable
      - PG_EXPORTER_AUTO_DISCOVER_DATABASES=true
      - PG_EXPORTER_EXCLUDE_DATABASES=template0,template1
    ports:
      - "127.0.0.1:9187:9187"
    depends_on:
      remnawave-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9187/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.25"
        reservations:
          memory: 32M
          cpus: "0.05"

  redis-exporter:
    image: oliver006/redis_exporter:v1.67.0-alpine
    container_name: redis-exporter
    hostname: redis-exporter
    profiles: ["monitoring"]
    <<: [*common, *logging]
    environment:
      - REDIS_ADDR=remnawave-redis:6379
      - REDIS_EXPORTER_CHECK_KEYS=*
    ports:
      - "127.0.0.1:9121:9121"
    depends_on:
      remnawave-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9121/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: "0.25"
        reservations:
          memory: 16M
          cpus: "0.05"

  node-exporter:
    image: prom/node-exporter:v1.9.1
    container_name: node-exporter
    hostname: node-exporter
    profiles: ["monitoring"]
    <<: [*common, *logging]
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/host"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
      - "--collector.netclass.ignored-devices=^(veth.*|docker.*|br-.*)"
    ports:
      - "127.0.0.1:9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host:ro,rslave
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.25"
        reservations:
          memory: 32M
          cpus: "0.05"

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.52.1
    container_name: cadvisor
    hostname: cadvisor
    profiles: ["monitoring"]
    <<: [*common, *logging]
    privileged: true
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.50"
        reservations:
          memory: 64M
          cpus: "0.10"

  # ============ MAILPIT CLUSTER (3 instances for OTP rotation testing) ============
  # Used for testing email provider rotation on OTP resend
  # 1st OTP request → mailpit-1 (SMTP: 1025, Web: 8025)
  # 2nd OTP request → mailpit-2 (SMTP: 1026, Web: 8026)
  # 3rd OTP request → mailpit-3 (SMTP: 1027, Web: 8027)
  # Then cycle repeats (round-robin)
  # Enable with: docker compose --profile email-test up -d

  mailpit-1:
    image: axllent/mailpit:v1.22
    container_name: mailpit-1
    hostname: mailpit-1
    profiles: ["email-test"]
    <<: [*common, *logging]
    environment:
      MP_SMTP_AUTH_ACCEPT_ANY: 1
      MP_SMTP_AUTH_ALLOW_INSECURE: 1
      MP_UI_AUTH_FILE: ""
      MP_MAX_MESSAGES: 5000
      MP_DATABASE: /data/mailpit.db
      MP_WEBROOT: /mailpit-1
    ports:
      - "127.0.0.1:8025:8025"   # Web UI
      - "127.0.0.1:1025:1025"   # SMTP
    volumes:
      - mailpit_1_data:/data
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8025/mailpit-1/api/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 3

  mailpit-2:
    image: axllent/mailpit:v1.22
    container_name: mailpit-2
    hostname: mailpit-2
    profiles: ["email-test"]
    <<: [*common, *logging]
    environment:
      MP_SMTP_AUTH_ACCEPT_ANY: 1
      MP_SMTP_AUTH_ALLOW_INSECURE: 1
      MP_UI_AUTH_FILE: ""
      MP_MAX_MESSAGES: 5000
      MP_DATABASE: /data/mailpit.db
      MP_WEBROOT: /mailpit-2
    ports:
      - "127.0.0.1:8026:8025"   # Web UI
      - "127.0.0.1:1026:1025"   # SMTP
    volumes:
      - mailpit_2_data:/data
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8025/mailpit-2/api/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 3

  mailpit-3:
    image: axllent/mailpit:v1.22
    container_name: mailpit-3
    hostname: mailpit-3
    profiles: ["email-test"]
    <<: [*common, *logging]
    environment:
      MP_SMTP_AUTH_ACCEPT_ANY: 1
      MP_SMTP_AUTH_ALLOW_INSECURE: 1
      MP_UI_AUTH_FILE: ""
      MP_MAX_MESSAGES: 5000
      MP_DATABASE: /data/mailpit.db
      MP_WEBROOT: /mailpit-3
    ports:
      - "127.0.0.1:8027:8025"   # Web UI
      - "127.0.0.1:1027:1025"   # SMTP
    volumes:
      - mailpit_3_data:/data
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8025/mailpit-3/api/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  remnawave-network:
    name: remnawave-network
    driver: bridge
    external: false

volumes:
  remnawave-db-data:
    name: remnawave-db-data
  caddy-ssl-data:
    name: caddy-ssl-data
  grafana_data:
    name: grafana_data
  prometheus_data:
    name: prometheus_data
  alertmanager_data:
    name: alertmanager_data
  loki_data:
    name: loki_data
  promtail_positions:
    name: promtail_positions
  tempo_data:
    name: tempo_data
  mailpit_1_data:
    name: mailpit_1_data
  mailpit_2_data:
    name: mailpit_2_data
  mailpit_3_data:
    name: mailpit_3_data
